{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "## for integrating with BERT\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# https://www.geeksforgeeks.org/how-to-generate-word-embedding-using-bert/\n",
    "\n",
    "\n",
    "## for integrating with LLAMA\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# pipeline = transformers.pipeline( \n",
    "#     \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-Guard-2-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-Guard-2-8B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get lagged conversational turns\n",
    "def process_input_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['utter1'] = df['content']\n",
    "    df['utter2'] = df['content'].shift(-1)\n",
    "    df['utter_order'] = df['participant'] + ' ' + df['participant'].shift(-1)\n",
    "    return df\n",
    "\n",
    "### NEEDS TO BE UPDATED FOR BERT ###\n",
    "# Function to compute embeddings, but first checks if already in cache and if not, add them there afterward\n",
    "default_embedding_engine = \"text-embedding-ada-002\"  # text-embedding-ada-002 is recommended\n",
    "def get_embedding_with_cache(\n",
    "    text: str,\n",
    "    engine: str = default_embedding_engine\n",
    ") -> list:\n",
    "    # Skip if there is no text content for computing embedding\n",
    "    if text is None:\n",
    "        return None\n",
    "    if (text, engine) not in embedding_cache.keys():\n",
    "        # if not in cache, call API to get embedding\n",
    "        embedding_cache[(text, engine)] = openai.embeddings.create(input=[text], model=engine).data[0].embedding\n",
    "    return embedding_cache[(text, engine)]\n",
    "\n",
    "# Function to process and get embeddings/cosines for a single file\n",
    "def process_file(file_path, embedding_cache, default_embedding_engine):       \n",
    "    df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "    df = process_input_data(df)\n",
    "\n",
    "   # Create column of embeddings\n",
    "    for column in [\"utter1\", \"utter2\"]:\n",
    "        df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n",
    "    \n",
    "    # Create column of cosine similiarity\n",
    "    df[\"cosine_similarity\"] = df.apply(\n",
    "        lambda row: cosine_similarity(\n",
    "            np.array(row[\"utter1_embedding\"]).reshape(1, -1),\n",
    "            np.array(row[\"utter2_embedding\"]).reshape(1, -1)\n",
    "        )[0][0] if row[\"utter1_embedding\"] is not None and row[\"utter2_embedding\"] is not None else None,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "##########\n",
    "\n",
    "# Load or initialize the embedding cache, if none there (first time running), then create empty cache to build\n",
    "embedding_cache_path = \"data/bert_embedding_cache.pkl\"\n",
    "try:\n",
    "    with open(embedding_cache_path, \"rb\") as f:\n",
    "        embedding_cache = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "\n",
    "##########\n",
    "\n",
    "# Path to the folder containing the text files\n",
    "folder_path = \"./data/prepped_stan_small\"\n",
    "text_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
    "\n",
    "# Process each file and update the cache\n",
    "concatenated_df = pd.DataFrame()\n",
    "for file_name in tqdm(text_files, desc=\"Processing files\"):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = process_file(file_path, embedding_cache, default_embedding_engine)\n",
    "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "    # Save the updated embedding cache to disk after processing each file\n",
    "    with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "        pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "# concatenated_df now contains all the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-bert-align",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
